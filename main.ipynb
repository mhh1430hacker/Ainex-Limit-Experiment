{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a370a0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AINEX LAW PROJECT: Recursive Semantic Decay Analysis\n",
    "# ==============================================================================\n",
    "# Investigation of AI model collapse through recursive self-training\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Install necessary libraries (silent installation)\n",
    "!pip install transformers datasets sentence-transformers scipy scikit-learn accelerate > /dev/null\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress non-critical warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Verify GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Computing device: {device.upper()}\")\n",
    "if device == 'cpu':\n",
    "    print(\"⚠ Warning: CPU execution will be significantly slower than GPU\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Load Models and Data\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n[1/4] Loading pre-trained models...\")\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Load Sentence-BERT for semantic embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load Wikipedia corpus for baseline training\n",
    "print(\"[2/4] Loading Wikipedia training corpus...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Data Processing Classes\n",
    "# ==============================================================================\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset wrapper for text sequences\n",
    "    Tokenizes and prepares texts for model training\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.inputs = []\n",
    "        for text in texts:\n",
    "            if len(text) > 50:  # Filter for meaningful content\n",
    "                encoding = tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                self.inputs.append(encoding)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: value.squeeze(0) for key, value in self.inputs[index].items()}\n",
    "\n",
    "# Extract and prepare Wikipedia texts\n",
    "print(\"[3/4] Preparing training corpus...\")\n",
    "human_knowledge_texts = [item['text'] for item in dataset if len(item['text']) > 100][:400]\n",
    "print(f\"  → Selected {len(human_knowledge_texts)} texts from Wikipedia\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Core Processing Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model_on_texts(model, texts, epochs=3, learning_rate=5e-5):\n",
    "    \"\"\"\n",
    "    Train the model on given texts for specified epochs.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT2LMHeadModel instance\n",
    "        texts: List of text strings for training\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Adam optimizer learning rate\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    dataset = TextDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"  → Training for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        progress_bar = tqdm(dataloader, leave=False)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "def generate_texts_from_model(model, num_texts=400, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text sequences using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: GPT2LMHeadModel instance (in eval mode)\n",
    "        num_texts: Number of texts to generate\n",
    "        temperature: Sampling temperature (lower = more focused)\n",
    "    \n",
    "    Returns:\n",
    "        List of generated text strings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "\n",
    "    print(f\"  → Generating {num_texts} text samples...\")\n",
    "\n",
    "    # Diverse starting prompts\n",
    "    prompts = [\n",
    "        \"The history\", \"Science is\", \"War began\", \"The theory\", \"He was born\",\n",
    "        \"In 1990\", \"The system\", \"Water is\", \"Computers are\", \"The city\"\n",
    "    ] * (num_texts // 10 + 5)\n",
    "\n",
    "    batch_size = 10\n",
    "    for i in tqdm(range(0, num_texts, batch_size)):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        if not batch_prompts:\n",
    "            break\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=100,\n",
    "                do_sample=True,\n",
    "                top_k=40,\n",
    "                temperature=temperature,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "\n",
    "        for output_ids in outputs:\n",
    "            decoded_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            generated_texts.append(decoded_text)\n",
    "\n",
    "    return generated_texts[:num_texts]\n",
    "\n",
    "def calculate_semantic_volume(texts):\n",
    "    \"\"\"\n",
    "    Calculate the geometric volume of semantic embeddings using convex hull.\n",
    "    \n",
    "    This metric represents the diversity and coverage of semantic space.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "    \n",
    "    Returns:\n",
    "        Float representing the volume (0.0 if calculation fails)\n",
    "    \"\"\"\n",
    "    embeddings = embedder.encode(texts)\n",
    "\n",
    "    # Reduce to 3D for volume calculation\n",
    "    pca = PCA(n_components=3)\n",
    "    coordinates = pca.fit_transform(embeddings)\n",
    "\n",
    "    try:\n",
    "        convex_hull = ConvexHull(coordinates)\n",
    "        return convex_hull.volume\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. AINEX RECURSIVE DECAY EXPERIMENT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" AINEX LAW: SEMANTIC COLLAPSE EXPERIMENT \".center(60))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Measure baseline (human knowledge)\n",
    "print(\"\\n[BASELINE] Measuring human knowledge semantic volume...\")\n",
    "baseline_volume = calculate_semantic_volume(human_knowledge_texts)\n",
    "print(f\"  ✓ Human Knowledge Volume: {baseline_volume:.6f}\")\n",
    "\n",
    "# Step 2: First generation (train on human data, generate synthetic)\n",
    "print(\"\\n[GENERATION 1] Learning from human knowledge...\")\n",
    "train_model_on_texts(model, human_knowledge_texts, epochs=2)\n",
    "generation_1_texts = generate_texts_from_model(model, num_texts=400)\n",
    "generation_1_volume = calculate_semantic_volume(generation_1_texts)\n",
    "print(f\"  ✓ Generation 1 Volume: {generation_1_volume:.6f}\")\n",
    "\n",
    "# Step 3: Second generation (recursive: train on own outputs)\n",
    "print(\"\\n[GENERATION 2] Self-replication (training on synthetic data)...\")\n",
    "train_model_on_texts(model, generation_1_texts, epochs=3)\n",
    "generation_2_texts = generate_texts_from_model(model, num_texts=400)\n",
    "generation_2_volume = calculate_semantic_volume(generation_2_texts)\n",
    "print(f\"  ✓ Generation 2 Volume: {generation_2_volume:.6f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Results and Analysis\n",
    "# ==============================================================================\n",
    "\n",
    "# Calculate semantic collapse rate\n",
    "collapse_percentage = ((baseline_volume - generation_2_volume) / baseline_volume) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EXPERIMENT RESULTS \".center(60))\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBaseline (Human Knowledge) Volume : {baseline_volume:.6f}\")\n",
    "print(f\"Generation 1 Volume                : {generation_1_volume:.6f}\")\n",
    "print(f\"Generation 2 Volume                : {generation_2_volume:.6f}\")\n",
    "print(f\"Semantic Collapse Rate             : {collapse_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "if collapse_percentage > 0:\n",
    "    print(\"✓ RESULT: SEMANTIC COLLAPSE CONFIRMED\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  • The model lost semantic diversity through recursive self-training\")\n",
    "    print(\"  • The AINEX Law is mathematically demonstrated\")\n",
    "    print(\"  • AI models converge to lower-dimensional semantic spaces when\")\n",
    "    print(\"    trained recursively on their own outputs\")\n",
    "else:\n",
    "    print(\"⚠ RESULT: SEMANTIC EXPANSION (Hallucination/Noise)\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  • The model generated novel but incoherent semantic patterns\")\n",
    "    print(\"  • This suggests random hallucination rather than true learning\")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
